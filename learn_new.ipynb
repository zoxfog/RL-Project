{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c419d81a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n",
      "0.19.0\n",
      "cuda:0\n",
      "NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import imageio\n",
    "from collections import namedtuple, deque\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "print(torch.__version__ )\n",
    "import gym\n",
    "print(gym.__version__)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a378ca2",
   "metadata": {},
   "source": [
    "## NN Impelementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b3ebef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(7*7*64, 512)\n",
    "        self.fc2 = nn.Linear(512, actions)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5fd87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dueling_DQN(nn.Module):\n",
    "    def __init__(self,h, w,  actions):\n",
    "        super(Dueling_DQN, self).__init__()\n",
    "        self.actions = actions        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1_adv = nn.Linear(7*7*64, 512)\n",
    "        self.fc1_val = nn.Linear(7*7*64, 512)\n",
    "        self.fc2_adv = nn.Linear(512, self.actions)\n",
    "        self.fc2_val = nn.Linear(512, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        adv = self.relu(self.fc1_adv(x))\n",
    "        val = self.relu(self.fc1_val(x))\n",
    "        adv = self.fc2_adv(adv)\n",
    "        val = self.fc2_val(val).expand(x.size(0), self.actions)        \n",
    "        x = val + adv - adv.mean(1).unsqueeze(1).expand(x.size(0), self.actions)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3be0ea9",
   "metadata": {},
   "source": [
    "## Screen Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f10dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the image to 3x84x84 from 3x250x160\n",
    "def transform(screen):\n",
    "    processed_image = cv2.resize(screen, (84, 84))\n",
    "    processed_tensor = torch.tensor(processed_image.astype('uint8')/255).permute(2, 0 ,1)\n",
    "    return processed_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55253233",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4316e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, capacity, history_len = 3 ):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity \n",
    "        self.history_len = 4\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            self.buffer = self.buffer[1:]\n",
    "                                   \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)-1\n",
    "        output = []\n",
    "        tr = batch_size\n",
    "        \n",
    "        while tr>0:\n",
    "            i = random.randint(0, buffer_size)\n",
    "            output.append(self.buffer[i])\n",
    "            tr -=1\n",
    "            # get the history transitions\n",
    "            j = i\n",
    "            while j>0 and (i-j)<self.history_len and tr>0:\n",
    "                j-=1\n",
    "                output.append(self.buffer[j])\n",
    "                tr -=1                        \n",
    "        return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e1883",
   "metadata": {},
   "source": [
    "## E-greedy Action Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31ecdf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_action( state, epsilon,main_net, actions):\n",
    "    r = random.random()\n",
    "    if r > epsilon:\n",
    "        with torch.no_grad():\n",
    "            output = main_net(state).cpu()[0]\n",
    "            greedy_actions = np.where(output.numpy()==np.max(output.numpy() ))[0]\n",
    "            greedy_action =  random.choice(greedy_actions )          \n",
    "        action = greedy_action\n",
    "    else:\n",
    "        action = random.randint(0, actions - 1)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac68ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_MSE(output, target):\n",
    "    loss = torch.mean(torch.clamp((output - target), min=-1, max=1,)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079572b6",
   "metadata": {},
   "source": [
    "## NN update for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0f2a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(main_net,target_net, memory, batch_size, gamma):\n",
    "    if len(memory) <  batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = torch.cat(batch.action).to(device)\n",
    "    reward_batch = torch.cat(batch.reward).to(device)\n",
    "\n",
    "    Q_s_a = main_net(state_batch).gather(1, action_batch).double()\n",
    "    next_state_values = torch.zeros( batch_size, device=device).double()\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    next_Q_s_a = (next_state_values * gamma) + reward_batch\n",
    "            \n",
    "\n",
    "    # Compute the loss\n",
    "    if type(main_net)==type(DQN(0,0,0)):\n",
    "        loss= clipped_MSE(Q_s_a, next_Q_s_a) \n",
    "    if type(main_net)==type(Dueling_DQN(0,0,0)):\n",
    "        loss =  nn.MSELoss()(Q_s_a, next_Q_s_a)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107179f6",
   "metadata": {},
   "source": [
    "## Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e74fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(main_net,game):\n",
    "    HEIGHT = 84\n",
    "    WIDTH = 84\n",
    "\n",
    "    GAME = game\n",
    "    env = gym.make(GAME)    \n",
    "    s = str(env.action_space)\n",
    "    val = s.split('(', 1)[1].split(')')[0]\n",
    "    ACTIONS= int(val) \n",
    "    \n",
    "    \n",
    "    episodes = 5\n",
    "    exp_rewards_mean =[]\n",
    "    exp_rewards_top_std =[]\n",
    "    exp_rewards_bottom_std =[]\n",
    "    epsilon = 0.05\n",
    "    k = 1\n",
    "    e_rewards = []\n",
    "\n",
    "    #initilize\n",
    "    for e in range(episodes):\n",
    "        total_rewards = 0\n",
    "        total_steps = 0\n",
    "        step  = 0\n",
    "        screen = env.reset()\n",
    "        tr_current_screen = transform(screen )\n",
    "        done = False\n",
    "        # while not terminal\n",
    "        while done==False and step<5000:\n",
    "            \n",
    "            if step%k==0 :\n",
    "                action = ret_action(tr_current_screen,epsilon,main_net,ACTIONS)                 \n",
    "            if step%50==0:\n",
    "                action = ret_action(tr_current_screen,1,main_net,ACTIONS) \n",
    "                \n",
    "            last_screen, reward, done, info = env.step(action)\n",
    "            total_rewards+=reward   \n",
    "            \n",
    "            tr_last_screen= transform(last_screen)\n",
    "\n",
    "            next_state = tr_current_screen - tr_last_screen\n",
    "            \n",
    "            # update state\n",
    "            tr_current_screen = tr_last_screen\n",
    "            total_steps = total_steps + 1\n",
    "            step = step + 1        \n",
    "        e_rewards.append(total_rewards)\n",
    "    mean = np.mean(e_rewards)\n",
    "    std_top = np.mean(e_rewards) + np.std(e_rewards)\n",
    "    std_bottm = np.mean(e_rewards) - np.std(e_rewards)\n",
    "    \n",
    "    return mean, std_top, std_bottm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa19deb1",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcb9e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_new(directory, model, max_episodes, max_steps , game = 'Breakout-v0' ,\n",
    "              gamma = 0.99, batch_size = 32, optimizer = 'RMSprop', lr=0.00025, momentum = 0.95,\n",
    "              eps = 0.01,epsilon_decay = 0.98, buffer_capacity = 100000, history_length = 4 ):\n",
    "       \n",
    "    # The directory for the checkpoitns\n",
    "    DIRECTORY_NAME = directory\n",
    "    \n",
    "    # hyperparameters\n",
    "    HEIGHT = 84\n",
    "    WIDTH = 84\n",
    "    GAME = game\n",
    "    MAX_EPISODES = max_episodes\n",
    "    MAX_STEPS = max_steps\n",
    "    MODEL = model\n",
    "    \n",
    "    env = gym.make(GAME)\n",
    "    s = str(env.action_space)\n",
    "    val = s.split('(', 1)[1].split(')')[0]\n",
    "    ACTIONS= int(val)\n",
    "\n",
    "    GAMMA = gamma\n",
    "    BATCH_SIZE = batch_size\n",
    "    TARGET_UPDATE = 10000\n",
    "    \n",
    "    MAX_EPSILON = 0.9\n",
    "    MIN_EPSILON = 0.1\n",
    "    EPSILON_DECAY = epsilon_decay\n",
    "    CAPACITY = buffer_capacity\n",
    "       \n",
    "    OPTIMIZER = optimizer\n",
    "    LEARNING_RATE = lr\n",
    "    MOMENTUM = momentum\n",
    "    WEIGHT_DECAY = 0\n",
    "    EPS = eps\n",
    "\n",
    "    if MODEL == 'DQN':\n",
    "        memory = ReplayBuffer(CAPACITY)\n",
    "        main_net = DQN(HEIGHT, WIDTH, ACTIONS).double().to(device)\n",
    "        target_net = DQN(HEIGHT, WIDTH, ACTIONS).double().to(device)\n",
    "        target_net.eval()\n",
    "    if MODEL == 'Dueling':\n",
    "        memory = ReplayBuffer(CAPACITY)\n",
    "        main_net = Dueling_DQN(HEIGHT, WIDTH, ACTIONS).double().to(device)\n",
    "        target_net = Dueling_DQN(HEIGHT, WIDTH, ACTIONS).double().to(device)\n",
    "        target_net.eval()\n",
    "\n",
    "    if OPTIMIZER ==  \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(main_net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, eps =  EPS,weight_decay=WEIGHT_DECAY)\n",
    "    elif OPTIMIZER ==  \"SGD\":\n",
    "        optimizer = optim.SGD(main_net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, eps =  EPS,weight_decay=WEIGHT_DECAY)\n",
    "    elif OPTIMIZER ==  \"ADAM\":\n",
    "        optimizer = optim.Adam(main_net.parameters(), lr=LEARNING_RATE, eps =  EPS, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # hyperparameter dictionary\n",
    "    hyperparameter_dict = {\"model\": MODEL,\"gamma\":GAMMA,\"batch size\":BATCH_SIZE, \"target update\": TARGET_UPDATE,\n",
    "                       \"max epsilon\": MAX_EPSILON, \"min epsilon\": MIN_EPSILON, \"epsilon decay\": EPSILON_DECAY,\"game\":GAME,\n",
    "                          \"max steps\":MAX_STEPS, \"max episodes\": MAX_EPISODES}\n",
    "\n",
    "    # Replay buffer hyperparameters hyperparameter dictionary\n",
    "    rb_hyperparameter_dict ={\"capacity\":CAPACITY }\n",
    "\n",
    "    opt_hyperparameters = {\"optimizer\": OPTIMIZER, 'learning rate': LEARNING_RATE,\"momentum\": MOMENTUM,\n",
    "                         \"eps\": EPS , \"weight decay\":WEIGHT_DECAY}\n",
    "    hyperparameters = {**hyperparameter_dict, **rb_hyperparameter_dict, **opt_hyperparameters}\n",
    "    \n",
    "   # create a directory to save the neural network's checkpoint parameters and checkpoint info   \n",
    "    path = os.path.abspath(os.getcwd())\n",
    "    dir_path = os.path.join(path, DIRECTORY_NAME)    \n",
    "    try:\n",
    "        os.mkdir(dir_path)\n",
    "    except:\n",
    "        print(\"Directory already exists\")\n",
    "    check_points = pd.DataFrame()\n",
    "\n",
    "    # initiliaze\n",
    "    total_steps = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "    episode_num = 0\n",
    "    training_loss = np.array([])            \n",
    "    k = 4     #skipped frames\n",
    "    noop = 30    # noop actions\n",
    "    \n",
    "    # expected reward \n",
    "    exp_rewards = []\n",
    "\n",
    "    # initliaze time managment variables\n",
    "    state_transformer_time = 0\n",
    "    env_step_time = 0\n",
    "    buffer_push_time = 0\n",
    "    net_update_time = 0\n",
    "    target_net_log_time = 0 \n",
    "    total_time = 0\n",
    "\n",
    "    while total_steps<MAX_STEPS and episode_num<MAX_EPISODES:\n",
    "        episode_num+=1\n",
    "        screen = env.reset()\n",
    "        tr_current_screen = transform(screen )        \n",
    "        total_rewards = 0\n",
    "        step  = 0\n",
    "        done =False\n",
    "\n",
    "        # while not terminal\n",
    "        while done==False:\n",
    "            \n",
    "            start_time = time.time()  \n",
    "                 \n",
    "            # take action in the current env\n",
    "            if step%k==0:\n",
    "                if step>noop and total_steps>30000:\n",
    "                    action = ret_action(tr_current_screen, epsilon,main_net, ACTIONS) \n",
    "                elif step>noop:\n",
    "                    action =  ret_action(tr_current_screen, 1,main_net, ACTIONS) \n",
    "                else:\n",
    "                    action = 0\n",
    "\n",
    "            next_screen, reward, done, info = env.step(action)\n",
    "            total_rewards+=reward \n",
    "            \n",
    "            # balance the rewards\n",
    "            if reward>=1:\n",
    "                reward = 1.0\n",
    "            time_checkpoint1 = time.time()\n",
    "            \n",
    "\n",
    "            if not done:\n",
    "                # transform the state to a 3x84x84 grey scale image   \n",
    "                tr_next_screen= transform(next_screen)\n",
    "            else:\n",
    "                tr_next_screen = None\n",
    "            time_checkpoint2 = time.time()\n",
    "\n",
    "            # add transition to the buffer\n",
    "            memory.push(tr_current_screen, torch.tensor([[action]]),\n",
    "                        tr_next_screen, torch.tensor([reward]))\n",
    "            time_checkpoint3 = time.time()\n",
    "\n",
    "            # compute the loss and run a single update\n",
    "            loss = update_model(main_net,target_net, memory, BATCH_SIZE, GAMMA )\n",
    "            if loss is not None:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                for param in main_net.parameters():\n",
    "                    param.grad.data.clamp_(max = 10)\n",
    "                optimizer.step()           \n",
    "                training_loss = np.append(training_loss, loss.item())\n",
    "            time_checkpoint4 = time.time()\n",
    "\n",
    "            # update state\n",
    "            tr_current_screen = tr_next_screen\n",
    "            \n",
    "            \n",
    "            total_steps = total_steps + 1\n",
    "            step = step + 1        \n",
    "            time_checkpoint5 = time.time()\n",
    "\n",
    "            # update the target weights \n",
    "            if step % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(main_net.state_dict())\n",
    "            time_checkpoint6 = time.time()\n",
    "\n",
    "            # update epsilon\n",
    "            if total_steps%10000==0:\n",
    "                if epsilon> MIN_EPSILON:\n",
    "                    epsilon*=EPSILON_DECAY\n",
    "            time_checkpoint7 = time.time()\n",
    "\n",
    "            # add elapsed time in minutes\n",
    "            env_step_time += (time_checkpoint1 - start_time)/60\n",
    "            state_transformer_time += (time_checkpoint2 - time_checkpoint1)/60\n",
    "            buffer_push_time += (time_checkpoint3-time_checkpoint2)/60\n",
    "            net_update_time += (time_checkpoint4-time_checkpoint3)/60\n",
    "            target_net_log_time += (time_checkpoint6-time_checkpoint5)/60 \n",
    "            total_time += (time_checkpoint7 - start_time)/60        \n",
    "\n",
    "            if done:\n",
    "                print(\"Episode \"+ str(episode_num)+\", timestep \"+str(total_steps) +\n",
    "                      \", finished after {} timesteps\".format(step+1))\n",
    "\n",
    "                # checkpoint: save model and info\n",
    "                checkpoint_episode = 2\n",
    "                if episode_num%checkpoint_episode==0 and episode_num>0:\n",
    "                    avg_reward = np.mean(exp_rewards)\n",
    "                    std_reward = np.std(exp_rewards)\n",
    "                    exp_rewards = []\n",
    "\n",
    "                    nn_name = './main_net'+str(episode_num )+'.pth'\n",
    "                    training_loss_name = '//training loss' +str(episode_num )+ '.npy'                                              \n",
    "\n",
    "                    check_point = {\"episode num\":episode_num  ,\"step\":total_steps ,\"epsilon\": epsilon,\n",
    "                                   \"avg reward\":avg_reward,\"std reward\":std_reward,\"path\":DIRECTORY_NAME+nn_name,\n",
    "                                  \"training loss path\": DIRECTORY_NAME+training_loss_name}\n",
    "\n",
    "                    avg_elapsed_time = {'state transformer time': state_transformer_time/checkpoint_episode,\n",
    "                                        'env step time': env_step_time/checkpoint_episode,\n",
    "                                        'buffer push time': buffer_push_time/checkpoint_episode,\n",
    "                                        'net update time': net_update_time /checkpoint_episode,\n",
    "                                        'target net update time': target_net_log_time/checkpoint_episode,\n",
    "                                        'avg time': total_time/checkpoint_episode}\n",
    "\n",
    "                    mean,top_std,bottom_std = simulation(main_net,GAME)\n",
    "                    scores = {'mean':mean,'top std':top_std,'bottom_std':bottom_std}\n",
    "\n",
    "                    # join the checkpoint dictionary and the hyperparameters data\n",
    "                    check_point ={**check_point, **hyperparameters,**avg_elapsed_time,**scores}\n",
    "                    check_points = check_points.append(check_point,ignore_index=True)\n",
    "                    check_points.to_excel(DIRECTORY_NAME + \"//checkpoints.xlsx\",index = False) \n",
    "                    torch.save(main_net.state_dict(), DIRECTORY_NAME+nn_name)\n",
    "                    np.save(DIRECTORY_NAME + training_loss_name,training_loss)\n",
    "\n",
    "                    state_transformer_time = 0\n",
    "                    env_step_time = 0\n",
    "                    buffer_push_time = 0\n",
    "                    net_update_time = 0\n",
    "                    target_net_log_time = 0 \n",
    "                    total_time = 0\n",
    "                break\n",
    "        exp_rewards.append(total_rewards) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19d14e",
   "metadata": {},
   "source": [
    "## Run Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70663c16",
   "metadata": {},
   "outputs": [],
   "source": [
    " directory = 'dir11_Dueling'\n",
    "model = 'Dueling'\n",
    "max_episodes = 10000\n",
    "max_steps = 5000000\n",
    "game = 'Breakout-v0'\n",
    "gamma = 0.99\n",
    "batch_size =32\n",
    "optimizer = 'ADAM'\n",
    "lr = 0.00025\n",
    "momentum = 0.95\n",
    "eps = 0.01\n",
    "epsilon_decay = 0.99\n",
    "buffer_capacity = 100000\n",
    "history_length = 4\n",
    "\n",
    "learn_new(directory, model, max_episodes, max_steps , game  ,\n",
    "              gamma , batch_size , optimizer , lr, momentum ,\n",
    "              eps ,epsilon_decay, buffer_capacity , history_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1720097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
